"""
Agentæ ¸å¿ƒæ¨¡å—
å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œåè°ƒLLMè°ƒç”¨ï¼Œç›®å‰åªåšç›´è¿LLMå’Œæ—¥å¿—è®°å½•
"""
import asyncio
import json
from typing import Dict, Any, Optional, AsyncGenerator
from datetime import datetime

from llm_interface import get_llm_interface, LLMResponse
from config import get_config
from logger import get_logger
from tool_registry import get_tools, execute_tool, ToolError, to_openai_tools

logger = get_logger()

# M3ç¼–æ’å™¨ç›¸å…³å¯¼å…¥
try:
    from orchestrator import get_orchestrator, orchestrate_query, OrchestratorResult
    M3_AVAILABLE = True
except ImportError:
    M3_AVAILABLE = False
    logger.warning("M3ç¼–æ’å™¨æ¨¡å—ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼")


class AgentCore:
    """Agentæ ¸å¿ƒç±»"""

    def __init__(self, llm_interface=None, use_m3: bool = False):
        self.config = get_config()
        self.llm = llm_interface or get_llm_interface()
        self.use_m3 = use_m3 and M3_AVAILABLE

        # åˆå§‹åŒ–å·¥å…·ç³»ç»Ÿ
        if self.config.tools_enabled:
            self.tools = get_tools()
            logger.info(f"å·¥å…·ç³»ç»Ÿå·²åŠ è½½: {len(self.tools)} ä¸ªå·¥å…·")
        else:
            self.tools = []
            logger.info("å·¥å…·ç³»ç»Ÿå·²ç¦ç”¨")

        # åˆå§‹åŒ–M3ç¼–æ’å™¨
        if self.use_m3:
            self.orchestrator = get_orchestrator()
            logger.info("M3ç¼–æ’å™¨æ¨¡å¼å·²å¯ç”¨")
        else:
            logger.info("ä¼ ç»ŸAgentæ¨¡å¼å·²å¯ç”¨")

        logger.info("Agentæ ¸å¿ƒåˆå§‹åŒ–å®Œæˆ", extra={
            "provider": self.config.model_provider,
            "rag_enabled": self.config.rag_enabled,
            "tools_enabled": self.config.tools_enabled,
            "tools_count": len(self.tools),
            "use_m3": self.use_m3,
            "llm_initialized": self.llm.provider is not None
        })

    def set_llm_interface(self, llm_interface) -> None:
        """
        è®¾ç½®LLMæ¥å£

        Args:
            llm_interface: LLMæ¥å£å®ä¾‹
        """
        self.llm = llm_interface
        logger.info("LLMæ¥å£å·²æ›´æ–°", extra={
            "provider": self.config.model_provider,
            "llm_initialized": self.llm.provider is not None
        })

    async def process(self, user_input: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥

        Args:
            user_input: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆé¢„ç•™ï¼‰

        Returns:
            DictåŒ…å«å“åº”å†…å®¹å’Œå…ƒæ•°æ®
        """
        if self.llm.provider is None:
            raise RuntimeError("LLMæä¾›è€…æœªåˆå§‹åŒ–ã€‚è¯·å…ˆè°ƒç”¨ set_llm_interface() è®¾ç½®æœ‰æ•ˆçš„LLMæ¥å£ï¼Œæˆ–è°ƒç”¨ llm.initialize_provider()")

        start_time = datetime.now()
        logger.info(f"å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥: {user_input[:100]}{'...' if len(user_input) > 100 else ''}")

        try:
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨M3æ¨¡å¼
            if self.use_m3 and hasattr(self, 'orchestrator'):
                logger.info("ä½¿ç”¨M3ç¼–æ’å™¨æ¨¡å¼å¤„ç†æŸ¥è¯¢")
                return await self._process_with_m3(user_query=user_input, context=context)

            # åˆå§‹åŒ–å¯¹è¯å†å²
            messages = []
            tool_call_trace = []
            max_tool_calls = 2  # æœ€å¤š2æ¬¡å·¥å…·è°ƒç”¨

            # ç³»ç»Ÿæç¤ºè¯
            system_prompt = self._build_system_prompt()
            messages.append({"role": "system", "content": system_prompt})

            # ç”¨æˆ·è¾“å…¥
            messages.append({"role": "user", "content": user_input})

            # å·¥å…·è°ƒç”¨å›è·¯
            for tool_call_count in range(max_tool_calls + 1):
                logger.info(f"å¼€å§‹ç¬¬ {tool_call_count + 1} è½®å¯¹è¯")

                # å‡†å¤‡å·¥å…·æ¨¡å¼
                tools_schema = None
                if self.tools and tool_call_count < max_tool_calls:
                    tools_schema = to_openai_tools(self.tools)
                    logger.debug(f"å¯ç”¨å·¥å…·æ¨¡å¼ï¼Œå…± {len(self.tools)} ä¸ªå·¥å…·")
                    logger.debug(f"å·¥å…·schema: {tools_schema}")
                    logger.debug(f"å½“å‰æ¶ˆæ¯åˆ—è¡¨: {messages}")

                # è°ƒç”¨LLMï¼ˆä¼ é€’messagesè€Œä¸æ˜¯promptï¼‰
                try:
                    llm_response = await self.llm.generate(
                        messages=messages,  # ä¼ é€’æ¶ˆæ¯åˆ—è¡¨
                        tools_schema=tools_schema
                    )
                except Exception as e:
                    logger.error(f"LLMè°ƒç”¨å¼‚å¸¸: {e}")
                    # å¦‚æœæ˜¯ç½‘ç»œé”™è¯¯ï¼Œé‡è¯•
                    if "timeout" in str(e).lower() or "connection" in str(e).lower():
                        logger.info("æ£€æµ‹åˆ°ç½‘ç»œé”™è¯¯ï¼Œç­‰å¾…åé‡è¯•...")
                        import asyncio
                        await asyncio.sleep(2)
                        continue
                    else:
                        raise e

                # å¤„ç†LLMå“åº”
                assistant_message = {"role": "assistant", "content": llm_response.content}

                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                if llm_response.function_calls:
                    logger.info(f"æ£€æµ‹åˆ° {len(llm_response.function_calls)} ä¸ªå·¥å…·è°ƒç”¨")

                    # æ„å»ºæ­£ç¡®çš„tool_callsæ ¼å¼
                    tool_calls = []
                    for tool_call in llm_response.function_calls:
                        if hasattr(tool_call, 'function'):
                            # OpenAIå¯¹è±¡æ ¼å¼ï¼Œè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                            tool_calls.append({
                                "id": tool_call.id,
                                "type": "function",
                                "function": {
                                    "name": tool_call.function.name,
                                    "arguments": tool_call.function.arguments if isinstance(tool_call.function.arguments, str) else json.dumps(tool_call.function.arguments)
                                }
                            })
                        else:
                            # å·²ç»æ˜¯å­—å…¸æ ¼å¼ï¼Œè½¬æ¢ä¸ºæ­£ç¡®æ ¼å¼
                            if "function" not in tool_call:
                                # éœ€è¦æ„å»ºfunctionå­—æ®µ
                                tool_calls.append({
                                    "id": tool_call.get("id", ""),
                                    "type": tool_call.get("type", "function"),
                                    "function": {
                                        "name": tool_call.get("name", ""),
                                        "arguments": tool_call.get("arguments", {}) if isinstance(tool_call.get("arguments"), str) else json.dumps(tool_call.get("arguments", {}))
                                    }
                                })
                            else:
                                tool_calls.append(tool_call)

                    assistant_message["tool_calls"] = tool_calls
                    messages.append(assistant_message)

                    # æ‰§è¡Œå·¥å…·è°ƒç”¨
                    tool_results = []
                    for tool_call in llm_response.function_calls:
                        tool_result = await self._execute_tool_call(tool_call)
                        tool_results.append(tool_result)

                        # æ·»åŠ å·¥å…·ç»“æœæ¶ˆæ¯
                        tool_message = {
                            "role": "tool",
                            "content": str(tool_result["result"]),
                            "tool_call_id": tool_call["id"]
                        }
                        messages.append(tool_message)

                        # è®°å½•å·¥å…·è°ƒç”¨è½¨è¿¹
                        if hasattr(tool_call, 'function'):
                            # OpenAIå¯¹è±¡æ ¼å¼
                            tool_name = tool_call.function.name
                            arguments = tool_call.function.arguments
                        else:
                            # å­—å…¸æ ¼å¼
                            tool_name = tool_call.get("name", "")
                            arguments = tool_call.get("arguments", {})

                        tool_call_trace.append({
                            "tool_name": tool_name,
                            "arguments": arguments,
                            "result": tool_result["result"],
                            "error": tool_result.get("error"),
                            "execution_time": tool_result["execution_time"]
                        })

                    # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œç»§ç»­ä¸‹ä¸€è½®å¯¹è¯
                    continue
                else:
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›æœ€ç»ˆç­”æ¡ˆ
                    messages.append(assistant_message)
                    break

            # è®¡ç®—æ€»å¤„ç†æ—¶é—´
            total_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"å¤„ç†æ€»æ—¶é—´: {total_time:.2f}s")

            # æ„å»ºè¿”å›ç»“æœ
            result = {
                "response": llm_response.content,
                "metadata": {
                    "model": llm_response.model,
                    "response_time": llm_response.response_time,
                    "total_time": total_time,
                    "usage": llm_response.usage,
                    "function_calls": llm_response.function_calls,
                    "tool_call_trace": tool_call_trace,
                    "conversation_rounds": len([m for m in messages if m["role"] == "assistant"]),
                    "timestamp": start_time.isoformat()
                }
            }

            return result

        except Exception as e:
            error_msg = f"å¤„ç†ç”¨æˆ·è¾“å…¥æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            logger.error(error_msg, exc_info=True)

            return {
                "response": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                "metadata": {
                    "error": str(e),
                    "timestamp": start_time.isoformat(),
                    "total_time": (datetime.now() - start_time).total_seconds()
                }
            }

    async def process_stream(self, user_input: str, context: Optional[Dict[str, Any]] = None) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼å¤„ç†ç”¨æˆ·è¾“å…¥

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Yields:
            Dict[str, Any]: æµå¼æ•°æ®å—
        """
        if self.llm.provider is None:
            raise RuntimeError("LLMæä¾›è€…æœªåˆå§‹åŒ–ã€‚è¯·å…ˆè°ƒç”¨ set_llm_interface() è®¾ç½®æœ‰æ•ˆçš„LLMæ¥å£ï¼Œæˆ–è°ƒç”¨ llm.initialize_provider()")

        start_time = datetime.now()
        logger.info(f"å¼€å§‹æµå¼å¤„ç†ç”¨æˆ·è¾“å…¥: {user_input[:100]}{'...' if len(user_input) > 100 else ''}")

        try:
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨M3æ¨¡å¼
            if self.use_m3 and hasattr(self, 'orchestrator'):
                logger.info("ä½¿ç”¨M3ç¼–æ’å™¨æ¨¡å¼å¤„ç†æŸ¥è¯¢")
                async for chunk in self._process_with_m3_stream(user_query=user_input, context=context):
                    yield chunk
                return

            # åˆå§‹åŒ–å¯¹è¯å†å²
            messages = []
            tool_call_trace = []
            max_tool_calls = 2  # æœ€å¤š2æ¬¡å·¥å…·è°ƒç”¨

            # ç³»ç»Ÿæç¤ºè¯
            system_prompt = self._build_system_prompt()
            messages.append({"role": "system", "content": system_prompt})

            # ç”¨æˆ·è¾“å…¥ï¼ˆåªæ·»åŠ ä¸€æ¬¡ï¼‰
            messages.append({"role": "user", "content": user_input})

            # å·¥å…·è°ƒç”¨å›è·¯
            for tool_call_count in range(max_tool_calls + 1):
                logger.info(f"å¼€å§‹ç¬¬ {tool_call_count + 1} è½®å¯¹è¯")

                # å‡†å¤‡å·¥å…·æ¨¡å¼
                tools_schema = None
                if self.tools and tool_call_count < max_tool_calls:
                    tools_schema = to_openai_tools(self.tools)
                    logger.debug(f"å¯ç”¨å·¥å…·æ¨¡å¼ï¼Œå…± {len(self.tools)} ä¸ªå·¥å…·")

                # yieldå·¥å…·è°ƒç”¨çŠ¶æ€
                if tools_schema:
                    yield {
                        "type": "status",
                        "status": "thinking",
                        "message": f"ğŸ¤” æ­£åœ¨æ€è€ƒå’Œåˆ†ææ‚¨çš„è¯·æ±‚..."
                    }

                # è°ƒç”¨LLMï¼ˆæµå¼æ¨¡å¼ï¼‰
                try:
                    full_content = ""
                    function_calls = []

                    async for chunk in self.llm.generate_stream(
                        messages=messages,
                        tools_schema=tools_schema
                    ):
                        if chunk.get("type") == "content":
                            # å¢é‡å†…å®¹
                            yield {
                                "type": "content",
                                "content": chunk["content"],
                                "full_content": chunk["full_content"]
                            }
                            full_content = chunk["full_content"]

                        elif chunk.get("type") == "final":
                            # æœ€ç»ˆç»“æœ
                            response = chunk["response"]
                            full_content = response.content
                            function_calls = response.function_calls or []
                            break

                        elif chunk.get("type") == "error":
                            # é”™è¯¯
                            yield {
                                "type": "error",
                                "error": chunk["error"]
                            }
                            return

                    # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                    if function_calls:
                        logger.info(f"æ£€æµ‹åˆ° {len(function_calls)} ä¸ªå·¥å…·è°ƒç”¨")

                        # yieldå·¥å…·è°ƒç”¨çŠ¶æ€
                        yield {
                            "type": "status",
                            "status": "tool_call",
                            "message": f"ğŸ”§ æ­£åœ¨è°ƒç”¨å·¥å…·..."
                        }

                        # å¤„ç†å·¥å…·è°ƒç”¨
                        tool_results = []
                        for tool_call in function_calls:
                            try:
                                # å¤„ç†æ–°çš„OpenAIæ ¼å¼
                                if tool_call.get("type") == "function" and "function" in tool_call:
                                    func_info = tool_call["function"]
                                    tool_name = func_info.get("name")
                                    tool_args = func_info.get("arguments", {})
                                    if isinstance(tool_args, str):
                                        try:
                                            tool_args = json.loads(tool_args)
                                        except json.JSONDecodeError:
                                            tool_args = {}
                                    tool_call_id = tool_call.get("id")
                                else:
                                    # å…¼å®¹æ—§æ ¼å¼
                                    tool_name = tool_call.get("name")
                                    tool_args = tool_call.get("arguments", {})
                                    tool_call_id = tool_call.get("id")

                                # æ›´æ–°tool_callæ ¼å¼ç”¨äºåç»­å¤„ç†
                                normalized_tool_call = {
                                    "id": tool_call_id,
                                    "name": tool_name,
                                    "arguments": tool_args
                                }

                                logger.info(f"æ‰§è¡Œå·¥å…·: {tool_name}, å‚æ•°: {tool_args}")

                                # yieldå…·ä½“å·¥å…·è°ƒç”¨çŠ¶æ€
                                yield {
                                    "type": "status",
                                    "status": "tool_executing",
                                    "message": f"ğŸ”§ æ­£åœ¨æ‰§è¡Œ {tool_name}..."
                                }

                                # ç‰¹æ®Šå¤„ç†ask_userå·¥å…·
                                if tool_name == "ask_user":
                                    # ask_userå·¥å…·éœ€è¦ç”¨æˆ·äº¤äº’ï¼Œè¿”å›ç‰¹æ®ŠçŠ¶æ€
                                    result = await self._execute_tool_call(normalized_tool_call)

                                    # yield ask_userçŠ¶æ€ï¼Œè®©å‰ç«¯æ˜¾ç¤ºè¾“å…¥ç•Œé¢
                                    yield {
                                        "type": "ask_user",
                                        "question": tool_args.get("question", "è¯·æä¾›æ›´å¤šä¿¡æ¯"),
                                        "context": tool_args.get("context", ""),
                                        "tool_call": normalized_tool_call
                                    }
                                    # ä¸å†ç»§ç»­å¤„ç†ï¼Œè¿”å›ç­‰å¾…ç”¨æˆ·è¾“å…¥
                                    return

                                # æ‰§è¡Œæ™®é€šå·¥å…·
                                result = await self._execute_tool_call(normalized_tool_call)

                                tool_results.append({
                                    "tool_call": normalized_tool_call,
                                    "result": result
                                })

                                tool_call_trace.append({
                                    "tool_name": tool_name,
                                    "args": tool_args,
                                    "result": result,
                                    "success": True
                                })

                                # yieldå·¥å…·æ‰§è¡Œç»“æœ
                                yield {
                                    "type": "tool_result",
                                    "tool_name": tool_name,
                                    "result": result
                                }

                            except Exception as e:
                                error_msg = f"å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {str(e)}"
                                logger.error(error_msg)

                                tool_results.append({
                                    "tool_call": normalized_tool_call,
                                    "error": str(e)
                                })

                                tool_call_trace.append({
                                    "tool_name": tool_name,
                                    "args": tool_args,
                                    "error": str(e),
                                    "success": False
                                })

                        # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯ï¼ˆåŒ…å«tool_callsï¼‰
                        messages.append({
                            "role": "assistant",
                            "content": full_content,
                            "tool_calls": function_calls  # function_callså·²ç»æ˜¯æ­£ç¡®çš„OpenAIæ ¼å¼
                        })

                        # æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯å†å²ï¼ˆå¿…é¡»åœ¨assistantæ¶ˆæ¯ä¹‹åï¼‰
                        for tool_result in tool_results:
                            if "result" in tool_result:
                                messages.append({
                                    "role": "tool",
                                    "content": str(tool_result["result"]),
                                    "tool_call_id": tool_result["tool_call"].get("id")
                                })
                            else:
                                messages.append({
                                    "role": "tool",
                                    "content": f"å·¥å…·æ‰§è¡Œå¤±è´¥: {tool_result['error']}",
                                    "tool_call_id": tool_result["tool_call"].get("id")
                                })

                        # yieldç»§ç»­æ€è€ƒçŠ¶æ€
                        yield {
                            "type": "status",
                            "status": "thinking",
                            "message": f"ğŸ’­ åŸºäºå·¥å…·ç»“æœç»§ç»­æ€è€ƒ..."
                        }

                        continue  # ç»§ç»­ä¸‹ä¸€è½®å¯¹è¯

                    else:
                        # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸå¯¹è¯
                        break

                except Exception as e:
                    logger.error(f"LLMè°ƒç”¨å¼‚å¸¸: {e}")
                    # å¦‚æœæ˜¯ç½‘ç»œé”™è¯¯ï¼Œé‡è¯•
                    if "timeout" in str(e).lower() or "connection" in str(e).lower():
                        logger.info("æ£€æµ‹åˆ°ç½‘ç»œé”™è¯¯ï¼Œç­‰å¾…åé‡è¯•...")
                        import asyncio
                        await asyncio.sleep(2)
                        continue
                    else:
                        yield {
                            "type": "error",
                            "error": f"LLMè°ƒç”¨å¼‚å¸¸: {str(e)}"
                        }
                        return

            # è®¡ç®—å“åº”æ—¶é—´
            response_time = (datetime.now() - start_time).total_seconds()

            # yieldæœ€ç»ˆç»“æœ
            yield {
                "type": "final",
                "response": full_content,
                "metadata": {
                    "mode": "traditional",
                    "tool_call_trace": tool_call_trace,
                    "response_time": response_time,
                    "total_rounds": tool_call_count + 1
                }
            }

        except Exception as e:
            logger.error(f"æµå¼å¤„ç†å¤±è´¥: {e}")
            yield {
                "type": "error",
                "error": f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            }

    async def simple_chat(self, user_input: str, context: Optional[Dict[str, Any]] = None) -> str:
        """ç®€å•é—®ç­”æ¥å£ - ç›´æ¥ç”¨Chatæ¨¡å‹å›ç­”"""
        if self.llm.provider is None:
            raise RuntimeError("LLMæä¾›è€…æœªåˆå§‹åŒ–")

        try:
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œè¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä¿æŒç®€æ´æ˜äº†ã€‚"},
                {"role": "user", "content": user_input}
            ]

            response = await self.llm.generate(
                messages=messages,
                max_tokens=1000,
                temperature=0.7
            )

            return response.content.strip()

        except Exception as e:
            logger.error(f"ç®€å•é—®ç­”å¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"

    async def _process_with_m3_stream(self, user_query: str, context: Optional[Dict[str, Any]] = None):
        """ä½¿ç”¨M3ç¼–æ’å™¨å¤„ç†æŸ¥è¯¢ï¼ˆçœŸÂ·æµå¼ï¼šassistant_contentè¿›èŠå¤©æ°”æ³¡ï¼Œå…¶ä»–äº‹ä»¶è¿›çŠ¶æ€æ ï¼‰"""
        if not hasattr(self, 'orchestrator') or not self.orchestrator:
            yield {"type": "error", "message": "M3ç¼–æ’å™¨æœªåˆå§‹åŒ–"}
            return

        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç»­è·‘åœºæ™¯ï¼ˆcontextåŒ…å«user_answerï¼‰
            is_resume = context and "user_answer" in context

            if is_resume:
                # ç»­è·‘åœºæ™¯ï¼šç»§ç»­æ‰§è¡Œå·²æœ‰çš„ä»»åŠ¡
                print("[DEBUG] æ£€æµ‹åˆ°ç»­è·‘åœºæ™¯ï¼Œå¼€å§‹ç»§ç»­æ‰§è¡Œä»»åŠ¡")
                yield {"type": "status", "message": "æ­£åœ¨ç»§ç»­æ‰§è¡Œä»»åŠ¡"}

                session_id = context.get("session_id")
                user_answer = context.get("user_answer")

                if session_id:
                    from orchestrator import get_session
                    session = get_session(session_id)

                    if session.active_task:
                        # ä½¿ç”¨ç°æœ‰çš„ä»»åŠ¡çŠ¶æ€ç»§ç»­æ‰§è¡Œ
                        # å°†ç”¨æˆ·ç­”æ¡ˆæ·»åŠ åˆ°execution_stateä¸­
                        if session.active_task.execution_state:
                            # æŸ¥æ‰¾ç­‰å¾…ç”¨æˆ·è¾“å…¥çš„æ­¥éª¤ï¼Œå¹¶è®¾ç½®ç­”æ¡ˆ
                            ask_user_pending = session.active_task.execution_state.get_artifact("ask_user_pending")
                            if ask_user_pending and isinstance(ask_user_pending, dict):
                                output_key = ask_user_pending.get("output_key", "user_location")
                                session.active_task.execution_state.set_artifact(output_key, user_answer)
                                # æ¸…é™¤ask_user_pendingçŠ¶æ€ï¼Œé˜²æ­¢é‡å¤è¯¢é—®
                                session.active_task.execution_state.set_artifact("ask_user_pending", None)
                                print(f"[DEBUG] è®¾ç½®ç”¨æˆ·ç­”æ¡ˆåˆ° {output_key}: {user_answer}ï¼Œæ¸…é™¤ask_user_pendingçŠ¶æ€")

                        # ä½¿ç”¨ç°æœ‰çš„active_taskç»§ç»­ç¼–æ’
                        result = await self.orchestrator.orchestrate(user_query="", context=context, active_task=session.active_task)
                    else:
                        # æ²¡æœ‰æ´»è·ƒä»»åŠ¡ï¼Œé‡æ–°è§„åˆ’
                        result = await self.orchestrator.orchestrate(user_query="", context=context)
                else:
                    result = await self.orchestrator.orchestrate(user_query="", context=context)
            else:
                # æ­£å¸¸è§„åˆ’åœºæ™¯
                print("[DEBUG] å‘é€çŠ¶æ€äº‹ä»¶: æ­£åœ¨è§„åˆ’ä»»åŠ¡")
                yield {"type": "status", "message": "æ­£åœ¨è§„åˆ’ä»»åŠ¡"}

                # æ¨¡æ‹Ÿè§„åˆ’è¿‡ç¨‹çš„æ€ç»´é“¾
                planning_steps = [
                    "åˆ†æç”¨æˆ·æŸ¥è¯¢æ„å›¾...",
                    "åˆ¶å®šæ‰§è¡Œè®¡åˆ’...",
                    "å‡†å¤‡è°ƒç”¨ç›¸å…³å·¥å…·...",
                    "ä¼˜åŒ–æ‰§è¡Œæ­¥éª¤..."
                ]

                for step in planning_steps:
                    print(f"[DEBUG] å‘é€æ€ç»´é“¾: {step}")
                    yield {"type": "assistant_content", "content": step + " "}
                    await asyncio.sleep(0.1)

                # æ‰§è¡Œç¼–æ’
                result = await self.orchestrator.orchestrate(user_query=user_query, context=context)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”¨æˆ·è¾“å…¥
            if result.status == "ask_user" and result.pending_questions:
                question = result.pending_questions[0]
                print(f"[DEBUG] å‘é€ ask_user äº‹ä»¶: {question}")
                yield {"type": "ask_user", "question": question, "context": "éœ€è¦ç”¨æˆ·ä¿¡æ¯"}
                return  # ç­‰å¾…ç”¨æˆ·è¾“å…¥ï¼Œä¸è¦ç»§ç»­æ‰§è¡Œ

            # æ£€æŸ¥æ˜¯å¦å¤„äºç­‰å¾…ç”¨æˆ·è¾“å…¥çŠ¶æ€
            if result.status == "waiting_for_user" and result.pending_questions:
                question = result.pending_questions[0]
                print(f"[DEBUG] å‘é€ ask_user äº‹ä»¶ (waitingçŠ¶æ€): {question}")
                yield {"type": "ask_user", "question": question, "context": "éœ€è¦ç”¨æˆ·ä¿¡æ¯"}
                return  # ç­‰å¾…ç”¨æˆ·è¾“å…¥ï¼Œä¸è¦ç»§ç»­æ‰§è¡Œ

            # çŠ¶æ€ï¼šå¼€å§‹æ‰§è¡Œ
            print("[DEBUG] å‘é€çŠ¶æ€äº‹ä»¶: æ­£åœ¨æ‰§è¡Œä»»åŠ¡")
            yield {"type": "status", "message": "æ­£åœ¨æ‰§è¡Œä»»åŠ¡"}

            # æ¨¡æ‹Ÿæ‰§è¡Œè¿‡ç¨‹çš„æ€ç»´é“¾
            if result.final_plan and result.final_plan.steps:
                for i, step in enumerate(result.final_plan.steps, 1):
                    step_type_str = step.type.value if hasattr(step.type, 'value') else str(step.type)
                    print(f"[DEBUG] å‘é€å·¥å…·è½¨è¿¹: æ‰§è¡Œæ­¥éª¤ {i}: {step_type_str}")
                    yield {"type": "tool_trace", "message": f"æ‰§è¡Œæ­¥éª¤ {i}: {step_type_str}"}

                    # æ ¹æ®æ­¥éª¤ç±»å‹æ˜¾ç¤ºä¸åŒçš„æ‰§è¡Œä¿¡æ¯
                    if step.type == "tool_call" and hasattr(step, 'tool') and step.tool:
                        print(f"[DEBUG] å‘é€å·¥å…·è°ƒç”¨: è°ƒç”¨å·¥å…· {step.tool}")
                        yield {"type": "assistant_content", "content": f"\nè°ƒç”¨å·¥å…· {step.tool}... "}
                    elif step.type == "web_search":
                        print(f"[DEBUG] å‘é€ç½‘ç»œæœç´¢: æœç´¢ {step.inputs.get('query', '')}")
                        yield {"type": "assistant_content", "content": f"\næ‰§è¡Œç½‘ç»œæœç´¢... "}
                    elif step.type == "ask_user":
                        print(f"[DEBUG] å‘é€ç”¨æˆ·è¯¢é—®: {step.inputs.get('question', '')}")
                        yield {"type": "assistant_content", "content": f"\néœ€è¦ç”¨æˆ·ä¿¡æ¯... "}
                    elif step.type == "summarize":
                        print(f"[DEBUG] å‘é€æ•°æ®æ±‡æ€»: {step.expect}")
                        yield {"type": "assistant_content", "content": f"\næ±‡æ€»åˆ†ææ•°æ®... "}

                    await asyncio.sleep(0.1)

            # å¤„ç†æœ€ç»ˆç»“æœ
            final_content = ""

            if result.final_answer:
                # æœ‰æ˜ç¡®çš„æœ€ç»ˆç­”æ¡ˆ
                final_content = "\n\n" + result.final_answer
            elif result.status == "completed" and hasattr(result, 'execution_state') and result.execution_state:
                # ä»»åŠ¡å®Œæˆä½†æ²¡æœ‰æ˜ç¡®ç­”æ¡ˆï¼Œä»artifactsä¸­æ„é€ æœ‰æ„ä¹‰çš„å“åº”
                artifacts = result.execution_state.artifacts
                response_parts = []

                # æå–æœ‰ç”¨çš„ä¿¡æ¯
                if "weather_data" in artifacts:
                    weather = artifacts["weather_data"]
                    if hasattr(weather, 'data') and weather.data:
                        response_parts.append(f"å¤©æ°”ä¿¡æ¯: {weather.data}")
                    elif isinstance(weather, dict):
                        response_parts.append(f"å¤©æ°”ä¿¡æ¯: {weather}")

                if "commute_tips" in artifacts:
                    tips = artifacts["commute_tips"]
                    if isinstance(tips, str):
                        response_parts.append(f"é€šå‹¤å»ºè®®: {tips}")
                    elif isinstance(tips, list):
                        response_parts.append("é€šå‹¤å»ºè®®:\n" + "\n".join(f"- {tip}" for tip in tips))

                if "file_path" in artifacts:
                    response_parts.append("æ–‡ä»¶å·²ä¿å­˜åˆ°æŒ‡å®šä½ç½®")

                if response_parts:
                    final_content = "\n\n" + "\n\n".join(response_parts)
                else:
                    final_content = "\n\nä»»åŠ¡å·²å®Œæˆï¼Œæ‰€æœ‰æ­¥éª¤éƒ½å·²æˆåŠŸæ‰§è¡Œã€‚"
            else:
                # å…¶ä»–çŠ¶æ€ï¼ˆå¦‚ask_user/waitingï¼‰ï¼Œä¸å‘èŠå¤©æ°”æ³¡å†™å…¥è¯¯å¯¼æ€§æ–‡æœ¬
                final_content = ""

            # çœŸÂ·æµå¼è¾“å‡ºæœ€ç»ˆå†…å®¹
            if final_content:
                print(f"[DEBUG] å‘é€æœ€ç»ˆå†…å®¹: {final_content[:100]}...")
                yield {"type": "assistant_content", "content": final_content}

            # çŠ¶æ€ï¼šå¤„ç†å®Œæˆ
            print("[DEBUG] å‘é€çŠ¶æ€äº‹ä»¶: å¤„ç†å®Œæˆ")
            yield {"type": "status", "message": "å¤„ç†å®Œæˆ"}

        except Exception as e:
            logger.error(f"M3æµå¼å¤„ç†å¤±è´¥: {e}")
            yield {"type": "error", "message": f"å¤„ç†å¤±è´¥: {str(e)}"}

    async def _process_with_m3(self, user_query: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨M3ç¼–æ’å™¨å¤„ç†ç”¨æˆ·æŸ¥è¯¢

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        start_time = datetime.now()

        try:
            # ä½¿ç”¨ç¼–æ’å™¨å¤„ç†æŸ¥è¯¢
            orchestrator_result = await self.orchestrator.orchestrate(user_query, context)

            # å¦‚æœæ˜¯ask_userçŠ¶æ€ï¼Œç¡®ä¿sessionä¸­æœ‰active_task
            if orchestrator_result.status == "ask_user" and context and "session_id" in context:
                session_id = context["session_id"]
                from orchestrator import get_session
                session = get_session(session_id)

                # åˆ›å»ºactive_taskæ¥ä¿å­˜å½“å‰çŠ¶æ€
                if not session.active_task:
                    from orchestrator.orchestrator import ActiveTask
                    session.active_task = ActiveTask()
                    session.active_task.plan = orchestrator_result.final_plan
                    session.active_task.execution_state = orchestrator_result.execution_state
                    session.active_task.iteration_count = orchestrator_result.iteration_count
                    session.active_task.plan_iterations = orchestrator_result.plan_iterations
                    session.active_task.total_tool_calls = orchestrator_result.total_tool_calls
                    print(f"[DEBUG] ä¸ºsession {session_id} åˆ›å»ºäº†active_task")

            # æ£€æŸ¥æ˜¯å¦å¤„äºask_userçŠ¶æ€
            if orchestrator_result.status == "ask_user" and orchestrator_result.execution_state:
                ask_user_data = orchestrator_result.execution_state.get_artifact("ask_user_pending")
                if ask_user_data:
                    response = f"ğŸ¤” {ask_user_data['question']}\n\nè¯·å‘Šè¯‰æˆ‘æ‚¨çš„ç­”æ¡ˆï¼Œæˆ‘å°†ç»§ç»­ä¸ºæ‚¨å¤„ç†è¯·æ±‚ã€‚"
                    metadata = {
                        "mode": "m3_orchestrator",
                        "status": "ask_user",
                        "ask_user_pending": ask_user_data,
                        "final_plan": orchestrator_result.final_plan,
                        "execution_state": orchestrator_result.execution_state,
                        "timestamp": start_time.isoformat()
                    }
                    return {
                        "response": response,
                        "metadata": metadata
                    }

            # è½¬æ¢ç»“æœæ ¼å¼ä»¥ä¿æŒä¸ä¼ ç»Ÿæ¨¡å¼çš„å…¼å®¹æ€§
            response = orchestrator_result.final_answer or "å¤„ç†å®Œæˆï¼Œä½†æ— æ³•ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚"

            # æ„å»ºå…ƒæ•°æ®
            metadata = {
                "mode": "m3_orchestrator",
                "status": orchestrator_result.status,
                "iteration_count": orchestrator_result.iteration_count,
                "total_time": orchestrator_result.total_time,
                "timestamp": start_time.isoformat(),
                "plan_steps": len(orchestrator_result.final_plan.steps) if orchestrator_result.final_plan else 0,
                "execution_artifacts": len(orchestrator_result.execution_state.artifacts) if orchestrator_result.execution_state else 0,
                "judge_history": [jr.to_dict() for jr in orchestrator_result.judge_history]
            }

            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨è½¨è¿¹ï¼Œæ·»åŠ åˆ°å…ƒæ•°æ®ä¸­
            if orchestrator_result.execution_state:
                tool_trace = []
                for step_id in orchestrator_result.execution_state.completed_steps:
                    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´è¯¦ç»†çš„å·¥å…·è°ƒç”¨ä¿¡æ¯
                    tool_trace.append({
                        "step_id": step_id,
                        "status": "completed"
                    })
                metadata["tool_call_trace"] = tool_trace

            # å¦‚æœæ‰§è¡Œå¤±è´¥ï¼Œæ·»åŠ é”™è¯¯ä¿¡æ¯
            if orchestrator_result.error_message:
                metadata["error"] = orchestrator_result.error_message
                if "å¤±è´¥" in orchestrator_result.status:
                    response = f"å¤„ç†å¤±è´¥ï¼š{orchestrator_result.error_message}"

            logger.info(f"M3ç¼–æ’å™¨å¤„ç†å®Œæˆ: {orchestrator_result.status}, è€—æ—¶{orchestrator_result.total_time:.2f}s")

            return {
                "response": response,
                "metadata": metadata
            }

        except Exception as e:
            logger.error(f"M3ç¼–æ’å™¨å¤„ç†å¤±è´¥: {e}", exc_info=True)

            return {
                "response": "M3ç¼–æ’å™¨å¤„ç†å¤±è´¥ï¼Œå·²å›é€€åˆ°ä¼ ç»Ÿæ¨¡å¼ã€‚",
                "metadata": {
                    "mode": "m3_fallback",
                    "error": str(e),
                    "timestamp": start_time.isoformat(),
                    "total_time": (datetime.now() - start_time).total_seconds()
                }
            }

    def _build_system_prompt(self) -> str:
        """
        æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰

        Returns:
            ç³»ç»Ÿæç¤ºè¯
        """
        base_prompt = """ä½ æ˜¯AIä¸ªäººåŠ©ç†ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·å¤„ç†å„ç§ä»»åŠ¡ã€‚

ä½ å…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š
1. åŸºç¡€å¯¹è¯å’Œé—®ç­”
2. å·¥å…·è°ƒç”¨èƒ½åŠ› - ä½ å¯ä»¥è°ƒç”¨å·¥å…·æ¥è·å–å¤–éƒ¨ä¿¡æ¯"""

        if self.tools:
            tool_names = [tool.name for tool in self.tools]
            base_prompt += f"\n3. å¯ç”¨å·¥å…·: {', '.join(tool_names)}"

        base_prompt += """

INSTRUCTIONS:
You are a helpful AI assistant with access to tools. When a user asks a question that requires external information, you MUST call the appropriate tool.

TOOL USAGE RULES:
- For time-related questions (current time, day of week, date): Call time_now
- For weather queries: Call weather_get
- For math calculations: Call math_calc
- For calendar/schedule queries: Call calendar_read
- For email queries: Call email_list
- For web searches: Call web_search
- For file reading: Call file_read
- For file writing: Call file_write (supports path aliases like "æ¡Œé¢", "ä¸‹è½½", "æ–‡æ¡£")
- For asking user information (location, date, etc.): Call ask_user

IMPORTANT: Always call the appropriate tool when external information is needed. Do not provide generic responses."""

        return base_prompt

    def _build_prompt(self, user_input: str, context: Optional[Dict[str, Any]] = None) -> str:
        """
        æ„å»ºä¼ ç»Ÿæç¤ºè¯ï¼ˆå‘åå…¼å®¹ï¼‰

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            å®Œæ•´çš„æç¤ºè¯
        """
        system_prompt = self._build_system_prompt()

        if context:
            # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ åˆ°æç¤ºè¯ä¸­
            context_str = "\n".join([f"{k}: {v}" for k, v in context.items()])
            prompt = f"{system_prompt}\n\nä¸Šä¸‹æ–‡ä¿¡æ¯:\n{context_str}\n\nç”¨æˆ·: {user_input}"
        else:
            prompt = f"{system_prompt}\n\nç”¨æˆ·: {user_input}"

        return prompt

    async def _execute_tool_call(self, tool_call: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•ä¸ªå·¥å…·è°ƒç”¨

        Args:
            tool_call: å·¥å…·è°ƒç”¨ä¿¡æ¯

        Returns:
            å·¥å…·æ‰§è¡Œç»“æœ
        """
        import time
        start_time = time.time()

        try:
            # è°ƒè¯•ï¼šæ‰“å°tool_callå¯¹è±¡çš„ç±»å‹å’Œç»“æ„
            logger.debug(f"Tool call object type: {type(tool_call)}")
            logger.debug(f"Tool call object: {tool_call}")
            logger.debug(f"Tool call dir: {[attr for attr in dir(tool_call) if not attr.startswith('_')]}")

            # å¤„ç†OpenAIå·¥å…·è°ƒç”¨å¯¹è±¡
            if hasattr(tool_call, 'function'):
                # OpenAIå·¥å…·è°ƒç”¨å¯¹è±¡
                logger.debug(f"Tool call has function attribute: {hasattr(tool_call.function, 'name')}")
                tool_name = tool_call.function.name
                arguments = tool_call.function.arguments
                tool_call_id = tool_call.id
                logger.debug(f"Extracted from OpenAI object: name={tool_name}, id={tool_call_id}")
            elif isinstance(tool_call, dict):
                # å­—å…¸æ ¼å¼ï¼ˆä»llm_interfaceè½¬æ¢è€Œæ¥ï¼‰
                tool_name = tool_call.get("name", "")
                arguments = tool_call.get("arguments", {})
                tool_call_id = tool_call.get("id", "")
                logger.debug(f"Extracted from dict: name={tool_name}, id={tool_call_id}, dict_keys={list(tool_call.keys())}")
            else:
                # å­—å…¸æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
                tool_name = tool_call.get("function", {}).get("name", "")
                arguments = tool_call.get("function", {}).get("arguments", "{}")
                tool_call_id = tool_call.get("id", "")
                logger.debug(f"Extracted from legacy dict: name={tool_name}, id={tool_call_id}")

            # è§£æå‚æ•°ï¼ˆå¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼‰
            if isinstance(arguments, str):
                try:
                    import json
                    arguments = json.loads(arguments)
                except json.JSONDecodeError:
                    arguments = {}

            logger.info(f"æ‰§è¡Œå·¥å…·: {tool_name}, å‚æ•°: {arguments}")

            # æ‰§è¡Œå·¥å…·
            result = execute_tool(tool_name, self.tools, **arguments)

            execution_time = time.time() - start_time

            return {
                "tool_call_id": tool_call_id,
                "result": result,
                "execution_time": execution_time,
                "success": True
            }

        except ToolError as e:
            execution_time = time.time() - start_time
            logger.error(f"å·¥å…·æ‰§è¡Œå¤±è´¥: {e}")

            # è·å–tool_call_id
            if hasattr(tool_call, 'id'):
                tool_call_id = tool_call.id
            else:
                tool_call_id = tool_call.get("id", "")

            return {
                "tool_call_id": tool_call_id,
                "result": f"å·¥å…·è°ƒç”¨å¤±è´¥: {e.message}",
                "error": str(e),
                "execution_time": execution_time,
                "success": False,
                "retryable": e.retryable
            }

        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"å·¥å…·æ‰§è¡Œå¼‚å¸¸: {e}")

            # è·å–tool_call_id
            if hasattr(tool_call, 'id'):
                tool_call_id = tool_call.id
            else:
                tool_call_id = tool_call.get("id", "")

            return {
                "tool_call_id": tool_call_id,
                "result": f"å·¥å…·æ‰§è¡Œå¼‚å¸¸: {str(e)}",
                "error": str(e),
                "execution_time": execution_time,
                "success": False,
                "retryable": False
            }

    def _log_response(self, llm_response: LLMResponse, user_input: str):
        """
        è®°å½•LLMå“åº”ä¿¡æ¯

        Args:
            llm_response: LLMå“åº”å¯¹è±¡
            user_input: ç”¨æˆ·è¾“å…¥
        """
        # è®°å½•åŸºæœ¬ä¿¡æ¯
        logger.info("LLMå“åº”å®Œæˆ", extra={
            "model": llm_response.model,
            "response_time": ".2f",
            "content_length": len(llm_response.content),
            "has_function_calls": llm_response.function_calls is not None
        })

        # å¦‚æœå¯ç”¨DEBUGæ¨¡å¼ï¼Œè®°å½•æ›´å¤šè¯¦ç»†ä¿¡æ¯
        if self.config.log_level == "DEBUG":
            logger.debug("è¯¦ç»†å“åº”ä¿¡æ¯", extra={
                "user_input": user_input[:200] + "..." if len(user_input) > 200 else user_input,
                "response_content": llm_response.content[:500] + "..." if len(llm_response.content) > 500 else llm_response.content,
                "usage": llm_response.usage,
                "function_calls": llm_response.function_calls
            })

        # è®°å½•tokenä½¿ç”¨æƒ…å†µ
        if llm_response.usage:
            usage_info = []
            if "total_tokens" in llm_response.usage:
                usage_info.append(f"æ€»token: {llm_response.usage['total_tokens']}")
            if "prompt_tokens" in llm_response.usage:
                usage_info.append(f"è¾“å…¥token: {llm_response.usage['prompt_tokens']}")
            if "completion_tokens" in llm_response.usage:
                usage_info.append(f"è¾“å‡ºtoken: {llm_response.usage['completion_tokens']}")

            if usage_info:
                logger.info(f"Tokenä½¿ç”¨æƒ…å†µ: {' | '.join(usage_info)}")


# å…¨å±€å®ä¾‹ï¼ˆé»˜è®¤ä¸åˆå§‹åŒ–LLMï¼Œä¾›ç‹¬ç«‹æµ‹è¯•ä½¿ç”¨ï¼‰
agent_core = AgentCore()


def get_agent_core() -> AgentCore:
    """è·å–Agentæ ¸å¿ƒå®ä¾‹"""
    return agent_core


def create_agent_core_with_llm(use_m3: bool = False) -> AgentCore:
    """åˆ›å»ºå¸¦æœ‰LLMåˆå§‹åŒ–çš„Agentæ ¸å¿ƒå®ä¾‹"""
    from llm_interface import create_llm_interface_with_keys
    llm = create_llm_interface_with_keys()
    return AgentCore(llm_interface=llm, use_m3=use_m3)


async def process_message(user_input: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šå¤„ç†å•ä¸ªæ¶ˆæ¯

    Args:
        user_input: ç”¨æˆ·è¾“å…¥
        context: ä¸Šä¸‹æ–‡

    Returns:
        å¤„ç†ç»“æœ
    """
    return await agent_core.process(user_input, context)
